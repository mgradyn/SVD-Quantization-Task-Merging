"""
Command-line interface for SVD-Hybrid merging.

=== TUTORIAL: Using the CLI ===

This module provides the command-line interface for running SVD-Hybrid merging.
It supports configuration via command-line arguments and JSON config files.

=== BASIC USAGE ===

    python -m src.svd_hybrid.cli \\
        --tasks Cars DTD EuroSAT SUN397 \\
        --checkpoint-dir ./checkpoints \\
        --base-model-path ./base_model.pt \\
        --output-dir ./output

=== CONFIGURATION OPTIONS ===

The CLI accepts three types of configuration:

1. **Command-line arguments**: Direct control
   --energy-threshold 0.95
   --max-rank 64
   --weighting performance

2. **Main config file** (--config): JSON with all settings
   {"tasks": [...], "svd_energy_threshold": 0.95}

3. **Specialized configs**:
   --quantize-config: Quantization settings
   --load-config: Loading settings (for quantized inputs)

=== PIPELINE STEPS ===

The CLI runs through 9 steps:
1. Load task vectors from checkpoints
2. Load and combine masks (if provided)
3. Extract parameter information
4. Construct SVD bases
5. Compress task vectors
6. Compute task weights
7. Merge parameters
8. Create merged model
9. Compute diagnostics

=== OUTPUT FILES ===

After running, you'll find:
- output_dir/merged_state_dict.pt: The merged model
- output_dir/weights.json: Task weights used
- output_dir/clusters.json: Cluster assignments (if clustering)
- artifact_dir/: Bases, coefficients, diagnostics (if --store-artifacts)
"""

import argparse
import torch
import os
from typing import Optional, Dict
from .config import SVDHybridConfig
from .task_vector_loader import load_task_vectors, get_task_checkpoint_paths, get_parameter_names
from .mask_loader import load_task_masks, combine_masks
from .basis import construct_masked_basis
from .compress import compress_all_parameters
from .merge import merge_all_parameters, apply_merged_deltas, merge_with_clustering
from .storage import save_all_artifacts, save_merged_model
from .diagnostics import compute_all_diagnostics, print_diagnostics_summary, compute_compression_statistics, print_detailed_compression_report
from .weighting import compute_weights
from .clustering import cluster_tasks
from .mask_loader import apply_mask_to_tensor, get_unmasked_portion
from .task_vector_loader import load_checkpoint


def run_svd_hybrid_pipeline(config: SVDHybridConfig) -> Dict:
    """
    Run the complete SVD-Hybrid merging pipeline.
    
    This is the main entry point for programmatic use. It executes all
    9 steps of the SVD-Hybrid method:
    
    1. Load task vectors from checkpoints
    2. Load and combine masks
    3. Extract parameter information
    4. Construct SVD bases for each parameter
    5. Compress task vectors (project + quantize)
    6. Compute task weights
    7. Merge parameters (dequantize + average + reconstruct)
    8. Create merged model (base + deltas)
    9. Compute diagnostics and save results
    
    Args:
        config: SVDHybridConfig object with all parameters
        
    Returns:
        Dictionary containing:
            - merged_state_dict: The merged model state dict
            - diagnostics: Reconstruction errors and metrics
            - bases: SVD bases for all parameters
            - compressed: Compressed coefficients for all tasks
            
    Example:
        >>> config = SVDHybridConfig(
        ...     tasks=["Cars", "DTD"],
        ...     checkpoint_dir="./ckpts",
        ...     base_model_path="./base.pt"
        ... )
        >>> results = run_svd_hybrid_pipeline(config)
        >>> model.load_state_dict(results["merged_state_dict"])
    """
    # Print tutorial header
    print(f"\n{'='*80}")
    print(f"ğŸ“š SVD-HYBRID MODEL MERGING PIPELINE")
    print(f"{'='*80}")
    print(f"""
ğŸ¯ WHAT IS SVD-HYBRID?
   SVD-Hybrid is an advanced method for merging multiple fine-tuned models into one.
   It combines:
   â€¢ SVD (Singular Value Decomposition) - finds the most important directions
   â€¢ Quantization - compresses the model changes efficiently
   â€¢ Task Arithmetic - combines what different models learned
   
   The result: A single model that can perform multiple tasks!

ğŸ“‹ PIPELINE OVERVIEW (9 Steps):
   1. Load task vectors (what each model learned)
   2. Load and combine masks (which parameters matter)
   3. Extract parameter information
   4. Construct SVD bases (find important directions)
   5. Compress task vectors (reduce storage)
   6. Compute task weights (how much each task matters)
   7. Merge parameters (combine the learning)
   8. Create merged model
   9. Compute diagnostics (verify quality)
""")
    
    device = config.device if torch.cuda.is_available() else "cpu"
    print(f"ğŸ–¥ï¸  Device: {device}")
    print(f"ğŸ“ Tasks to merge: {config.tasks}")
    print(f"{'='*80}\n")
    
    # Load base model state dict
    print(f"{'â”€'*80}")
    print(f"ğŸ“‚ PRELIMINARY: Loading Base Model")
    print(f"{'â”€'*80}")
    print(f"   â””â”€ The base model is the starting point before any task-specific training")
    print(f"   â””â”€ Path: {config.base_model_path}")
    base_state_dict = load_checkpoint(config.base_model_path, device=device)
    num_base_params = len(base_state_dict)
    total_base_elements = sum(p.numel() for p in base_state_dict.values() if hasattr(p, 'numel'))
    print(f"   âœ… LOADED: {num_base_params:,} parameters, {total_base_elements:,} total elements")
    
    # =========================================================================
    # Step 1: Load task vectors
    # =========================================================================
    print(f"\n{'â”€'*80}")
    print(f"ğŸ“š STEP 1/9: Loading Task Vectors")
    print(f"{'â”€'*80}")
    print(f"""
   ğŸ¯ WHAT ARE TASK VECTORS?
      Each task vector represents what a model "learned" during fine-tuning:
      task_vector = fine_tuned_weights - base_weights
      
      By loading these, we can see exactly what changed for each task.
""")
    task_checkpoint_paths = get_task_checkpoint_paths(config.checkpoint_dir, config.tasks)
    print(f"   ğŸ“ Checkpoint directory: {config.checkpoint_dir}")
    print(f"   ğŸ“‹ Tasks to load: {config.tasks}")
    
    task_vectors = load_task_vectors(
        config.base_model_path,
        task_checkpoint_paths,
        device=device
    )
    
    # Validation
    print(f"\n   ğŸ”¬ VALIDATION:")
    if len(task_vectors) == len(config.tasks):
        print(f"   âœ… CHECK PASSED: All {len(task_vectors)} task vectors loaded successfully")
    else:
        print(f"   âš ï¸ WARNING: Expected {len(config.tasks)} tasks, got {len(task_vectors)}")
    
    # Show stats per task
    print(f"\n   ğŸ“Š TASK VECTOR STATISTICS:")
    for task_name, tv in task_vectors.items():
        total_norm = sum(d.norm().item()**2 for d in tv.values()) ** 0.5
        print(f"      â””â”€ {task_name}: {len(tv)} params, total change norm: {total_norm:.4f}")
    
    # =========================================================================
    # Step 2: Load and combine masks
    # =========================================================================
    print(f"\n{'â”€'*80}")
    print(f"ğŸ“š STEP 2/9: Loading and Combining Masks")
    print(f"{'â”€'*80}")
    print(f"""
   ğŸ¯ WHAT ARE MASKS?
      Masks identify which parameters are "important" for each task.
      By combining masks, we focus on parameters that matter across tasks.
      
      Strategy '{config.svd_mask_strategy}' will be used to combine them.
""")
    
    if config.mask_dir and os.path.exists(config.mask_dir):
        print(f"   ğŸ“ Mask directory: {config.mask_dir}")
        task_masks = load_task_masks(
            config.mask_dir, 
            config.tasks, 
            device=device,
            reference_state_dict=base_state_dict
        )
        combined_masks = combine_masks(task_masks, strategy=config.svd_mask_strategy, device=device)
        
        # Validation
        if len(combined_masks) > 0:
            total_masked = sum(m.sum().item() for m in combined_masks.values())
            total_elements = sum(m.numel() for m in combined_masks.values())
            mask_ratio = total_masked / max(total_elements, 1) * 100
            print(f"   âœ… LOADED: {len(combined_masks)} parameter masks")
            print(f"   ğŸ“Š Coverage: {mask_ratio:.1f}% of parameters are masked (considered important)")
        else:
            print(f"   âš ï¸ WARNING: No masks found or loaded")
    else:
        print(f"   â„¹ï¸ No mask directory provided - using all parameters")
        print(f"   â””â”€ This means all parameters will be considered equally")
        combined_masks = {}
    
    # =========================================================================
    # Step 3: Get parameter names and shapes
    # =========================================================================
    print(f"\n{'â”€'*80}")
    print(f"ğŸ“š STEP 3/9: Extracting Parameter Information")
    print(f"{'â”€'*80}")
    print(f"""
   ğŸ¯ WHY THIS STEP?
      We need to know the structure of all parameters to process them correctly.
      This ensures we handle each layer/weight matrix appropriately.
""")
    
    param_names = get_parameter_names(task_vectors)
    original_shapes = {}
    total_params = 0
    for task_vector in task_vectors.values():
        for param_name, delta in task_vector.items():
            if param_name not in original_shapes:
                original_shapes[param_name] = delta.shape
                total_params += delta.numel()
    
    print(f"   ğŸ“Š Found {len(param_names):,} unique parameters")
    print(f"   ğŸ“Š Total parameter elements: {total_params:,}")
    
    # Show sample of parameter names
    sample_params = list(param_names)[:5]
    print(f"   ğŸ“‹ Sample parameters: {sample_params}")
    if len(param_names) > 5:
        print(f"      ... and {len(param_names) - 5} more")
    
    print(f"   âœ… VALIDATION: Parameter info extracted successfully")
    
    # =========================================================================
    # Step 4: Construct SVD bases for each parameter
    # =========================================================================
    print(f"\n{'â”€'*80}")
    print(f"ğŸ“š STEP 4/9: Constructing SVD Bases")
    print(f"{'â”€'*80}")
    print(f"""
   ğŸ¯ WHAT IS SVD?
      SVD (Singular Value Decomposition) finds the "principal directions" of change.
      
      Think of it like this: if multiple tasks all change weights in similar ways,
      SVD finds those common patterns. This lets us:
      â€¢ Compress the representation (fewer numbers to store)
      â€¢ Focus on what matters (high-energy = important changes)
      
   âš™ï¸ Settings:
      â€¢ Energy threshold: {config.svd_energy_threshold} (keep {config.svd_energy_threshold*100}% of variance)
      â€¢ Max rank: {config.svd_max_rank} (limit complexity)
      â€¢ Center data: {config.svd_center}
""")
    
    # Print mathematical overview for SVD
    print(f"""
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                          SVD MATHEMATICAL PROCESS                            â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                                              â”‚
â”‚  ğŸ“ FOR EACH PARAMETER (weight matrix/bias vector):                         â”‚
â”‚                                                                              â”‚
â”‚  Step 1: Stack task deltas into matrix T                                    â”‚
â”‚          T = [Î´â‚ | Î´â‚‚ | ... | Î´â‚™] âˆˆ â„^(DÃ—N)                                â”‚
â”‚          where D = flattened parameter dimension                            â”‚
â”‚                N = number of tasks ({len(config.tasks)})                                 â”‚
â”‚                                                                              â”‚
â”‚  Step 2: Center the data (optional, enabled: {config.svd_center})                         â”‚
â”‚          T_centered = T - mean(T, axis=1)                                   â”‚
â”‚                                                                              â”‚
â”‚  Step 3: Compute SVD decomposition                                          â”‚
â”‚          T = U Ã— Î£ Ã— Váµ€                                                     â”‚
â”‚          â€¢ U âˆˆ â„^(DÃ—D): Left singular vectors (basis directions)           â”‚
â”‚          â€¢ Î£ = diag(Ïƒâ‚, Ïƒâ‚‚, ..., Ïƒâ‚˜): Singular values (importance)         â”‚
â”‚          â€¢ Váµ€ âˆˆ â„^(NÃ—N): Right singular vectors                            â”‚
â”‚                                                                              â”‚
â”‚  Step 4: Select rank k based on energy threshold ({config.svd_energy_threshold})               â”‚
â”‚          Find smallest k such that:                                         â”‚
â”‚          (Ïƒâ‚Â² + Ïƒâ‚‚Â² + ... + Ïƒâ‚–Â²) / (Ïƒâ‚Â² + Ïƒâ‚‚Â² + ... + Ïƒâ‚˜Â²) â‰¥ {config.svd_energy_threshold}         â”‚
â”‚                                                                              â”‚
â”‚  Step 5: Split basis                                                        â”‚
â”‚          U_high = U[:, :k]   â†’ Top k directions (high energy)              â”‚
â”‚          U_low = U[:, k:]    â†’ Remaining directions (low energy)           â”‚
â”‚                                                                              â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
""")
    
    bases = {}
    successful_bases = 0
    total_energy_retained = 0
    ranks_selected = []
    dimensions = []
    
    for i, param_name in enumerate(param_names):
        # Extract masked and unmasked deltas
        mask = combined_masks.get(param_name)
        
        masked_deltas = []
        unmasked_deltas = []
        
        for task_name, task_vector in task_vectors.items():
            if param_name not in task_vector:
                continue
            
            delta = task_vector[param_name]
            
            if mask is not None and mask.shape == delta.shape:
                # Apply mask
                if mask.sum() >= config.svd_min_mask_size:
                    masked = apply_mask_to_tensor(delta, mask)
                    masked_deltas.append(masked)
                    
                    if config.svd_include_noise:
                        unmasked = get_unmasked_portion(delta, mask)
                        unmasked_deltas.append(unmasked)
            else:
                # No mask, use entire parameter
                masked_deltas.append(delta.flatten())
        
        if masked_deltas and len(masked_deltas[0]) > 0:
            basis = construct_masked_basis(
                masked_deltas,
                unmasked_deltas if config.svd_include_noise else None,
                energy_threshold=config.svd_energy_threshold,
                max_rank=config.svd_max_rank,
                center=config.svd_center,
                device=device,
                include_noise=config.svd_include_noise
            )
            
            # Convert to FP16 if requested
            if config.svd_fp16 and basis.get("masked") is not None:
                basis["masked"]["U_high"] = basis["masked"]["U_high"].half()
                basis["masked"]["U_low"] = basis["masked"]["U_low"].half()
                
                if basis.get("noise") is not None:
                    basis["noise"]["U_high"] = basis["noise"]["U_high"].half()
                    basis["noise"]["U_low"] = basis["noise"]["U_low"].half()
            
            bases[param_name] = basis
            
            if basis.get("masked"):
                successful_bases += 1
                total_energy_retained += basis['masked']['energy_retained']
                ranks_selected.append(basis['masked']['k'])
                dimensions.append(basis['masked']['D'])
                
                # Print progress for significant parameters with more detail
                if i < 5 or i % 50 == 0:
                    k = basis['masked']['k']
                    D = basis['masked']['D']
                    energy = basis['masked']['energy_retained']
                    reduction = D / k if k > 0 else float('inf')
                    print(f"   â”œâ”€ {param_name[:35]}{'...' if len(param_name) > 35 else '':<5}")
                    print(f"   â”‚     â””â”€ D={D:,} â†’ k={k} ({reduction:.1f}x dim reduction), energy={energy:.4f}")
    
    # Validation and summary
    avg_energy = total_energy_retained / max(successful_bases, 1)
    avg_rank = sum(ranks_selected) / max(len(ranks_selected), 1)
    avg_dim = sum(dimensions) / max(len(dimensions), 1)
    total_original_dim = sum(dimensions)
    total_compressed_dim = sum(ranks_selected)
    
    print(f"""
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                           SVD BASIS CONSTRUCTION SUMMARY                     â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                                              â”‚
â”‚  ğŸ“Š CONSTRUCTION RESULTS:                                                   â”‚
â”‚     Bases constructed: {successful_bases}/{len(param_names)} parameters                              â”‚
â”‚     Total dimension (Î£ D): {total_original_dim:,} elements                              â”‚
â”‚     Total rank (Î£ k): {total_compressed_dim:,} elements                                  â”‚
â”‚     Dimensionality reduction: {total_original_dim/max(total_compressed_dim,1):.1f}x                                     â”‚
â”‚                                                                              â”‚
â”‚  ğŸ“ˆ RANK STATISTICS:                                                        â”‚
â”‚     Average rank k: {avg_rank:.1f}                                              â”‚
â”‚     Rank range: [{min(ranks_selected) if ranks_selected else 0}, {max(ranks_selected) if ranks_selected else 0}]                                                â”‚
â”‚     Average dimension D: {avg_dim:.1f}                                          â”‚
â”‚                                                                              â”‚
â”‚  âš¡ ENERGY RETENTION:                                                        â”‚
â”‚     Target threshold: {config.svd_energy_threshold*100:.1f}%                                           â”‚
â”‚     Achieved average: {avg_energy*100:.2f}%                                          â”‚
â”‚     Status: {'âœ… Target met' if avg_energy >= config.svd_energy_threshold - 0.05 else 'âš ï¸ Below target'}                                              â”‚
â”‚                                                                              â”‚
â”‚  ğŸ’¾ BASIS STORAGE FORMAT:                                                    â”‚
â”‚     U_high: {'FP16' if config.svd_fp16 else 'FP32'} (2 bytes/value) - High energy directions         â”‚
â”‚     U_low: {'FP16' if config.svd_fp16 else 'FP32'} (2 bytes/value) - Low energy directions          â”‚
â”‚                                                                              â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
""")
    
    # =========================================================================
    # Step 5: Compress task vectors
    # =========================================================================
    print(f"\n{'â”€'*80}")
    print(f"ğŸ“š STEP 5/9: Compressing Task Vectors")
    print(f"{'â”€'*80}")
    print(f"""
   ğŸ¯ WHY COMPRESS?
      After SVD, we have coefficients for each task. Now we quantize (compress)
      the less important coefficients to save space.
      
   âš™ï¸ Compression Settings:
      â€¢ Low-energy bits: {config.svd_low_bits} bits
      â€¢ RTVQ stages: {config.svd_rtvq_stages} (more stages = better quality)
      
   ğŸ’¡ How it works:
      â€¢ High-energy coefficients â†’ FP16 (preserve accuracy)
      â€¢ Low-energy coefficients â†’ {config.svd_low_bits}-bit RTVQ (save space)
""")
    
    compressed_all = compress_all_parameters(
        task_vectors,
        combined_masks,
        bases,
        config,
        device=device
    )
    
    print(f"   âœ… COMPRESSED: {len(compressed_all)} parameters")
    
    # Compute and print detailed compression statistics
    compression_stats = compute_compression_statistics(
        task_vectors,
        compressed_all,
        bases,
        config
    )
    print_detailed_compression_report(compression_stats, config)
    
    # =========================================================================
    # Step 6: Compute task weights
    # =========================================================================
    print(f"\n{'â”€'*80}")
    print(f"ğŸ“š STEP 6/9: Computing Task Weights")
    print(f"{'â”€'*80}")
    print(f"""
   ğŸ¯ WHY WEIGHT TASKS?
      Not all tasks are equally important or perform equally well.
      Weighting lets us give more influence to better-performing tasks.
      
   âš™ï¸ Weighting Strategy: '{config.svd_weighting}'
""")
    
    # Print mathematical explanation based on weighting strategy
    if config.svd_weighting == "uniform":
        print(f"""
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                           UNIFORM WEIGHTING FORMULA                          â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                                              â”‚
â”‚  ğŸ“ FORMULA:                                                                â”‚
â”‚     w_i = 1/N  for all tasks i âˆˆ {{1, 2, ..., N}}                           â”‚
â”‚                                                                              â”‚
â”‚     where N = {len(config.tasks)} (number of tasks)                                        â”‚
â”‚           w_i = 1/{len(config.tasks)} = {1.0/len(config.tasks):.4f}                                                  â”‚
â”‚                                                                              â”‚
â”‚  ğŸ“ Each task contributes equally to the merged model                       â”‚
â”‚     Constraint: Î£w_i = 1                                                    â”‚
â”‚                                                                              â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
""")
    elif config.svd_weighting == "performance":
        print(f"""
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                       PERFORMANCE-BASED WEIGHTING FORMULA                    â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                                              â”‚
â”‚  ğŸ“ FORMULA (softmax with temperature):                                     â”‚
â”‚     w_i = exp(p_i / Ï„) / Î£â±¼ exp(p_j / Ï„)                                    â”‚
â”‚                                                                              â”‚
â”‚     where p_i = performance (accuracy) of task i                            â”‚
â”‚           Ï„ = temperature = {config.svd_weighting_temperature} (higher = more uniform)                 â”‚
â”‚                                                                              â”‚
â”‚  ğŸ“ Tasks with higher accuracy get more weight                              â”‚
â”‚     Temperature controls how much we favor high performers                  â”‚
â”‚     Ï„ â†’ 0: winner-take-all, Ï„ â†’ âˆ: uniform weights                         â”‚
â”‚                                                                              â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
""")
    elif config.svd_weighting == "cluster":
        print(f"""
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                         CLUSTER-BASED WEIGHTING FORMULA                      â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                                              â”‚
â”‚  ğŸ“ TWO-STAGE WEIGHTING:                                                    â”‚
â”‚                                                                              â”‚
â”‚  Stage 1: Within-cluster averaging                                          â”‚
â”‚     c_k = Î£áµ¢âˆˆcluster_k (w_i Ã— Î´_i) / |cluster_k|                           â”‚
â”‚                                                                              â”‚
â”‚  Stage 2: Across-cluster averaging                                          â”‚
â”‚     merged = Î£â‚– w_cluster_k Ã— c_k                                           â”‚
â”‚                                                                              â”‚
â”‚  ğŸ“ Similar tasks are grouped and averaged first                            â”‚
â”‚     Then cluster results are combined                                       â”‚
â”‚     Number of clusters: {config.svd_cluster_k}                                              â”‚
â”‚                                                                              â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
""")
    
    cluster_assignments = None
    
    if config.svd_weighting == "cluster":
        print(f"   ğŸ”„ Clustering tasks into {config.svd_cluster_k} groups...")
        cluster_assignments = cluster_tasks(task_vectors, config.svd_cluster_k, method="kmeans")
        
        from .clustering import get_cluster_members
        clusters = get_cluster_members(cluster_assignments)
        for cluster_id, members in clusters.items():
            print(f"      â””â”€ Cluster {cluster_id}: {members}")
    
    weights = compute_weights(
        config.tasks,
        weighting_strategy=config.svd_weighting,
        performance_file=config.performance_file,
        temperature=config.svd_weighting_temperature,
        cluster_assignments=cluster_assignments
    )
    
    print(f"\n   ğŸ“Š TASK WEIGHTS:")
    total_weight = 0
    for task_name, weight in sorted(weights.items()):
        bar_len = int(weight * 40)
        bar = 'â–ˆ' * bar_len + 'â–‘' * (40 - bar_len)
        print(f"      {task_name:15s}: {weight:.4f} [{bar}]")
        total_weight += weight
    
    # Validation
    if abs(total_weight - 1.0) < 0.01:
        print(f"   âœ… VALIDATION: Weights sum to {total_weight:.4f} (should be ~1.0)")
    else:
        print(f"   âš ï¸ WARNING: Weights sum to {total_weight:.4f} (expected ~1.0)")
    
    # =========================================================================
    # Step 7: Merge parameters
    # =========================================================================
    print(f"\n{'â”€'*80}")
    print(f"ğŸ“š STEP 7/9: Merging Parameters")
    print(f"{'â”€'*80}")
    print(f"""
   ğŸ¯ THE MAGIC STEP!
      Now we combine all the task vectors into one merged representation.
      
   ğŸ’¡ Process:
      1. Dequantize the compressed coefficients
      2. Compute weighted average: merged = Î£(weight_i Ã— task_i)
      3. Reconstruct the parameter deltas from coefficients
""")
    
    # Print mathematical explanation for merging
    print(f"""
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                          MERGING MATHEMATICAL PROCESS                        â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                                              â”‚
â”‚  ğŸ“ FOR EACH PARAMETER, MERGING HAPPENS IN COEFFICIENT SPACE:               â”‚
â”‚                                                                              â”‚
â”‚  Step 1: DEQUANTIZE low-energy coefficients                                 â”‚
â”‚          c_low = dequant(qâ‚) + dequant(qâ‚‚) + ... (RTVQ reconstruction)     â”‚
â”‚                                                                              â”‚
â”‚  Step 2: WEIGHTED AVERAGE of coefficients across tasks                      â”‚
â”‚          c_high_merged = Î£áµ¢ (wáµ¢ Ã— c_high_i)                                 â”‚
â”‚          c_low_merged = Î£áµ¢ (wáµ¢ Ã— c_low_i)                                   â”‚
â”‚                                                                              â”‚
â”‚          where wáµ¢ = weight for task i (Î£wáµ¢ = 1)                             â”‚
â”‚                                                                              â”‚
â”‚  Step 3: RECONSTRUCT merged delta in parameter space                        â”‚
â”‚          Î´_merged = U_high Ã— c_high_merged + U_low Ã— c_low_merged          â”‚
â”‚                                                                              â”‚
â”‚          This reverses the SVD projection to get the merged delta          â”‚
â”‚                                                                              â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
""")
    
    if config.svd_weighting == "cluster" and cluster_assignments is not None:
        print(f"   â””â”€ Using cluster-based merging (hierarchical)")
        merged_deltas = merge_with_clustering(
            compressed_all,
            bases,
            combined_masks,
            weights,
            cluster_assignments,
            original_shapes,
            config,
            device=device
        )
    else:
        print(f"   â””â”€ Using direct weighted averaging")
        merged_deltas = merge_all_parameters(
            compressed_all,
            bases,
            combined_masks,
            weights,
            original_shapes,
            config,
            device=device
        )
    
    print(f"   âœ… MERGED: {len(merged_deltas)} parameters")
    
    # Compute merge statistics
    total_merge_norm = sum(d.norm().item()**2 for d in merged_deltas.values()) ** 0.5
    print(f"   ğŸ“Š Total merged delta norm: {total_merge_norm:.4f}")
    
    # =========================================================================
    # Step 8: Apply to base model
    # =========================================================================
    print(f"\n{'â”€'*80}")
    print(f"ğŸ“š STEP 8/9: Creating Merged Model")
    print(f"{'â”€'*80}")
    print(f"""
   ğŸ¯ FINAL ASSEMBLY!
      We add the merged deltas back to the base model:
      merged_model = base_model + merged_deltas
      
      This creates a single model that combines all task knowledge!
""")
    
    # Print mathematical explanation
    print(f"""
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                          FINAL MODEL ASSEMBLY FORMULA                        â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                                              â”‚
â”‚  ğŸ“ TASK ARITHMETIC FORMULA:                                                â”‚
â”‚     Î¸_merged = Î¸_base + Î± Ã— Î´_merged                                        â”‚
â”‚                                                                              â”‚
â”‚     where Î¸_base = base (pre-trained) model parameters                      â”‚
â”‚           Î´_merged = combined task vector from SVD-Hybrid                   â”‚
â”‚           Î± = scaling factor (typically 1.0)                                â”‚
â”‚                                                                              â”‚
â”‚  ğŸ“ The merged model inherits:                                              â”‚
â”‚     â€¢ Base knowledge from Î¸_base                                            â”‚
â”‚     â€¢ Combined task-specific adaptations from Î´_merged                      â”‚
â”‚                                                                              â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
""")
    
    # Make a copy of base state dict
    merged_state_dict = apply_merged_deltas(
        {k: v.clone() for k, v in base_state_dict.items()}, 
        merged_deltas, 
        device=device
    )
    
    print(f"   âœ… CREATED: Merged model with {len(merged_state_dict):,} parameters")
    
    # Validation: Check shapes match
    shape_match = all(
        merged_state_dict[k].shape == base_state_dict[k].shape
        for k in base_state_dict.keys()
    )
    if shape_match:
        print(f"   âœ… VALIDATION: All parameter shapes match base model")
    else:
        print(f"   âŒ ERROR: Some parameter shapes don't match!")
    
    # =========================================================================
    # Step 9: Compute diagnostics
    # =========================================================================
    print(f"\n{'â”€'*80}")
    print(f"ğŸ“š STEP 9/9: Computing Diagnostics")
    print(f"{'â”€'*80}")
    print(f"""
   ğŸ¯ QUALITY CHECK
      We measure how well we preserved the original task information
      after all the compression and merging.
""")
    
    if config.svd_eval_reconstruction:
        diagnostics = compute_all_diagnostics(
            task_vectors,
            compressed_all,
            bases,
            combined_masks,
            config,
            device=device
        )
        
        # Add weight information
        diagnostics["task_weights"] = weights
        if cluster_assignments:
            diagnostics["cluster_assignments"] = cluster_assignments
        
        print_diagnostics_summary(diagnostics)
        
        # Additional validation
        summary = diagnostics.get("summary", {})
        avg_error = summary.get("average_reconstruction_error", 0)
        
        if avg_error < 0.05:
            print(f"   âœ… QUALITY CHECK PASSED: Reconstruction error {avg_error:.4f} (<5%)")
        elif avg_error < 0.10:
            print(f"   âš ï¸ QUALITY CHECK WARNING: Reconstruction error {avg_error:.4f} (5-10%)")
        else:
            print(f"   âŒ QUALITY CONCERN: High reconstruction error {avg_error:.4f} (>10%)")
    else:
        diagnostics = {"task_weights": weights}
        print(f"   â„¹ï¸ Skipping reconstruction evaluation (disabled in config)")
    
    # Save artifacts if requested
    if config.svd_store_artifacts:
        print(f"\nğŸ“¦ Saving artifacts to {config.artifact_dir}...")
        save_all_artifacts(
            bases,
            compressed_all,
            diagnostics,
            config,
            config.artifact_dir
        )
        print(f"   âœ… Artifacts saved")
    
    # Save merged model
    print(f"\nğŸ’¾ Saving merged model to {config.output_dir}...")
    save_merged_model(merged_state_dict, config.output_dir)
    
    # Save weights and clusters separately for convenience
    import json
    os.makedirs(config.output_dir, exist_ok=True)
    
    with open(os.path.join(config.output_dir, "weights.json"), 'w') as f:
        json.dump(weights, f, indent=2)
    
    if cluster_assignments:
        with open(os.path.join(config.output_dir, "clusters.json"), 'w') as f:
            json.dump(cluster_assignments, f, indent=2)
    
    # Final summary
    print(f"\n{'='*80}")
    print(f"ğŸ‰ SVD-HYBRID MERGING COMPLETE!")
    print(f"{'='*80}")
    print(f"""
   ğŸ“Š FINAL SUMMARY:
   â”œâ”€ Tasks merged: {len(config.tasks)} ({', '.join(config.tasks)})
   â”œâ”€ Parameters processed: {len(merged_deltas):,}
   â”œâ”€ Average SVD rank: {avg_rank:.1f}
   â”œâ”€ Average energy retained: {avg_energy*100:.1f}%
   â”œâ”€ Merged model saved to: {config.output_dir}/merged_state_dict.pt
   â””â”€ Weighting: {config.svd_weighting}
   
   ğŸ’¡ NEXT STEPS:
   â€¢ Load the merged model: torch.load('{config.output_dir}/merged_state_dict.pt')
   â€¢ Run evaluation on each task to verify performance
   â€¢ Compare with individual fine-tuned models
""")
    print(f"{'='*80}\n")
    
    return {
        "merged_state_dict": merged_state_dict,
        "diagnostics": diagnostics,
        "bases": bases,
        "compressed": compressed_all
    }


def parse_args():
    """Parse command-line arguments."""
    parser = argparse.ArgumentParser(
        description="SVD-Hybrid merging method combining Tall Masks and TVQ"
    )
    
    # Config file options
    parser.add_argument("--config", type=str, default=None,
                       help="Path to JSON config file (overrides command-line args)")
    parser.add_argument("--quantize-config", type=str, default=None,
                       help="Path to quantization config JSON")
    parser.add_argument("--load-config", type=str, default=None,
                       help="Path to loading config JSON")
    
    # Task configuration
    parser.add_argument("--tasks", nargs="+",
                       help="List of task identifiers")
    parser.add_argument("--model", type=str, default="ViT-B-32",
                       help="Model identifier (e.g., ViT-B-32)")
    parser.add_argument("--checkpoint-dir", type=str,
                       help="Directory containing task checkpoints")
    parser.add_argument("--base-model-path", type=str,
                       help="Path to base model checkpoint")
    parser.add_argument("--mask-dir", type=str, default="",
                       help="Directory containing tall masks")
    
    # TVQ-specific loading options
    parser.add_argument("--load-tv-type", type=str, default=None,
                       choices=["standard", "quantized", "quantized_finetuned", "quantized_base_and_tv"],
                       help="Type of task vector to load")
    parser.add_argument("--load-task-bits", type=int, default=8,
                       help="Bits for task vector quantization when loading")
    parser.add_argument("--load-base-bits", type=int, default=8,
                       help="Bits for base model quantization when loading")
    
    # SVD parameters
    parser.add_argument("--energy-threshold", type=float, default=0.95,
                       help="Energy retention threshold for rank selection")
    parser.add_argument("--max-rank", type=int, default=64,
                       help="Maximum rank cap")
    parser.add_argument("--center", action="store_true", default=True,
                       help="Center task matrix before SVD")
    parser.add_argument("--no-center", action="store_false", dest="center",
                       help="Don't center task matrix")
    parser.add_argument("--fp16", action="store_true", default=True,
                       help="Use FP16 for bases")
    parser.add_argument("--no-fp16", action="store_false", dest="fp16",
                       help="Use FP32 for bases")
    
    # Quantization parameters
    parser.add_argument("--low-bits", type=int, default=4,
                       help="Bits for low-energy coefficient quantization")
    parser.add_argument("--rtvq-stages", type=int, default=2,
                       help="Number of RTVQ refinement stages")
    
    # Mask parameters
    parser.add_argument("--mask-strategy", type=str, default="union",
                       choices=["union", "intersection", "majority"],
                       help="Mask combination strategy")
    parser.add_argument("--include-noise", action="store_true",
                       help="Process unmasked (noise) region")
    parser.add_argument("--noise-shrink", type=float, default=0.5,
                       help="Shrinkage factor for noise region")
    
    # Weighting parameters
    parser.add_argument("--weighting", type=str, default="uniform",
                       choices=["uniform", "performance", "cluster"],
                       help="Task weighting strategy")
    parser.add_argument("--performance-file", type=str, default=None,
                       help="Path to performance metrics JSON file")
    parser.add_argument("--temperature", type=float, default=5.0,
                       help="Temperature for performance-based weighting")
    parser.add_argument("--cluster-k", type=int, default=2,
                       help="Number of clusters for cluster-based weighting")
    
    # Storage and evaluation
    parser.add_argument("--store-artifacts", action="store_true",
                       help="Store compression artifacts")
    parser.add_argument("--eval-reconstruction", action="store_true", default=True,
                       help="Evaluate reconstruction error")
    parser.add_argument("--no-eval-reconstruction", action="store_false",
                       dest="eval_reconstruction",
                       help="Skip reconstruction evaluation")
    
    # Output paths
    parser.add_argument("--output-dir", type=str, default="./svd_hybrid_output",
                       help="Output directory for merged model")
    parser.add_argument("--artifact-dir", type=str, default="./artifacts",
                       help="Directory for artifact storage")
    
    # Device
    parser.add_argument("--device", type=str, default="cuda",
                       help="Device to use (cuda or cpu)")
    
    return parser.parse_args()


def main():
    """Main CLI entry point."""
    args = parse_args()
    
    # Load JSON config if specified
    import json
    config_overrides = {}
    
    if args.config:
        with open(args.config, 'r') as f:
            config_overrides = json.load(f)
        print(f"Loaded config from {args.config}")
    
    if args.quantize_config:
        with open(args.quantize_config, 'r') as f:
            quant_config = json.load(f)
        print(f"Loaded quantization config from {args.quantize_config}")
        # Merge quantization settings
        if 'quantization' in quant_config:
            config_overrides.update(quant_config['quantization'])
        if 'tasks' in quant_config and not args.tasks:
            config_overrides['tasks'] = quant_config['tasks']
        if 'checkpoints' in quant_config:
            config_overrides.update(quant_config['checkpoints'])
    
    if args.load_config:
        with open(args.load_config, 'r') as f:
            load_cfg = json.load(f)
        print(f"Loaded loading config from {args.load_config}")
        # Merge loading settings
        if 'loading' in load_cfg:
            config_overrides.update(load_cfg['loading'])
        if 'tasks' in load_cfg and not args.tasks:
            config_overrides['tasks'] = load_cfg['tasks']
        if 'checkpoints' in load_cfg:
            config_overrides.update(load_cfg['checkpoints'])
    
    # Apply config file values, then override with command-line args
    tasks = config_overrides.get('tasks', args.tasks)
    checkpoint_dir = config_overrides.get('checkpoint_dir', args.checkpoint_dir)
    base_model_path = config_overrides.get('base_model_path', args.base_model_path)
    
    if not tasks:
        raise ValueError("--tasks must be specified either via command-line or config file")
    if not checkpoint_dir:
        raise ValueError("--checkpoint-dir must be specified either via command-line or config file")
    if not base_model_path:
        raise ValueError("--base-model-path must be specified either via command-line or config file")
    
    # Create config from args
    config = SVDHybridConfig(
        tasks=tasks,
        model=args.model,
        checkpoint_dir=checkpoint_dir,
        base_model_path=base_model_path,
        mask_dir=args.mask_dir,
        svd_energy_threshold=args.energy_threshold,
        svd_max_rank=args.max_rank,
        svd_center=args.center,
        svd_fp16=args.fp16,
        svd_low_bits=args.low_bits,
        svd_rtvq_stages=args.rtvq_stages,
        svd_mask_strategy=args.mask_strategy,
        svd_include_noise=args.include_noise,
        svd_noise_shrink=args.noise_shrink,
        svd_weighting=args.weighting,
        performance_file=args.performance_file,
        svd_weighting_temperature=args.temperature,
        svd_cluster_k=args.cluster_k,
        svd_store_artifacts=args.store_artifacts,
        svd_eval_reconstruction=args.eval_reconstruction,
        output_dir=args.output_dir,
        artifact_dir=args.artifact_dir,
        device=args.device
    )
    
    # Run pipeline
    results = run_svd_hybrid_pipeline(config)
    
    return results


if __name__ == "__main__":
    main()
