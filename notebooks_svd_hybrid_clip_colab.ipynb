{
  "nbformat": 4,
  "nbformat_minor": 5,
  "metadata": {
    "colab": {
      "name": "SVD-Hybrid-CLIP-Light-Experiment",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "intro"
      },
      "source": [
        "# SVD-Hybrid CLIP Light Experiment\n",
        "\n",
        "This Colab notebook runs the light (4-task) vision experiment for the SVD-Hybrid merging approach combining:\n",
        "- Task deltas from fine-tuned CLIP ViT-B/32 checkpoints (Cars, EuroSAT, DTD, SUN397)\n",
        "- SVD-based high/low energy basis split\n",
        "- 2-bit quantization (placeholder or via TVQ) of low-energy coefficients\n",
        "\n",
        "## What You Will Do\n",
        "1. Install dependencies (PyTorch, CLIP, optional TVQ)\n",
        "2. Clone your repository containing `svd_hybrid_clip/`\n",
        "3. Provide or simulate fine-tuned task checkpoints\n",
        "4. Run the light merging pipeline\n",
        "5. Inspect artifacts (bases, compressed coefficients, merged weights)\n",
        "6. (Optional) Visualize singular values & reconstruction error\n",
        "\n",
        "## Before You Start\n",
        "If you already have real fine-tuned checkpoints, upload them or mount Google Drive. Otherwise you can simulate them for a mechanical test."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gpu_check"
      },
      "source": [
        "## 1. GPU Check"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gpu",
        "executionInfo": {}
      },
      "source": [
        "!nvidia-smi || echo \"No GPU detected. Proceeding on CPU may be slow.\""
      ],
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "install"
      },
      "source": [
        "## 2. Install Dependencies\n",
        "If you have a specific CUDA version, adjust the PyTorch index URL. Below uses a common recent CUDA wheel mirror; if it fails, fallback to the default PyPI install."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "deps"
      },
      "source": [
        "%%bash\n",
        "pip install --upgrade pip\n",
        "# Try CUDA wheel (adjust if needed). If it fails, rerun without index-url.\n",
        "pip install torch torchvision --index-url https://download.pytorch.org/whl/cu121\n",
        "pip install git+https://github.com/openai/CLIP.git\n",
        "# Optional: TVQ (if public & accessible)\n",
        "pip install git+https://github.com/AIM-SKKU/TVQ.git || echo \"TVQ optional or unavailable.\""
      ],
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "repo_clone"
      },
      "source": [
        "## 3. Clone Your Repository\n",
        "Replace `mgradyn/svd-hybrid-clip` with your actual GitHub <owner>/<repo>. If the repo is private, configure a PAT or use Drive upload instead."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "clone"
      },
      "source": [
        "REPO_URL = \"https://github.com/mgradyn/svd-hybrid-clip.git\"  # CHANGE if needed\n",
        "!git clone $REPO_URL\n",
        "%cd svd-hybrid-clip || echo \"Change directory name if your repo differs.\""
      ],
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "drive_mount"
      },
      "source": [
        "## 4. (Optional) Mount Google Drive for Checkpoints\n",
        "If you already have fine-tuned CLIP checkpoints in Drive (e.g., in `MyDrive/clip_finetuned/`), copy them into the local `checkpoints/` directory.\n",
        "\n",
        "Skip this if you'll simulate checkpoints."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mount_drive"
      },
      "source": [
        "# Uncomment to mount Drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "!mkdir -p checkpoints\n",
        "# Example copy command (adjust path):\n",
        "# !cp /content/drive/MyDrive/clip_finetuned/*.pt checkpoints/ || echo \"No real checkpoints copied.\""
      ],
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "simulate"
      },
      "source": [
        "## 5. (Optional) Simulate Fine-Tuned Checkpoints\n",
        "If you do not have real task-specific checkpoints yet, simulate them by adding small random noise to base CLIP weights. **These are not meaningful for performance metrics**, but let you test the pipeline mechanics."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "simulate_code"
      },
      "source": [
        "import torch\n",
        "try:\n",
        "    import clip\n",
        "except ImportError:\n",
        "    raise ImportError(\"CLIP not installed. Re-run installation cell.\")\n",
        "\n",
        "model, preprocess = clip.load(\"ViT-B/32\", device=\"cpu\")\n",
        "base_sd = {k: v.clone() for k, v in model.state_dict().items() if k.startswith(\"visual.\")}\n",
        "\n",
        "tasks = [\"cars\", \"eurosat\", \"dtd\", \"sun397\"]\n",
        "for t in tasks:\n",
        "    task_sd = {}\n",
        "    for k, v in base_sd.items():\n",
        "        # Small perturbation simulating fine-tune differences\n",
        "        task_sd[k] = v + 0.01 * torch.randn_like(v)\n",
        "    torch.save(task_sd, f\"checkpoints/{t}_clip_vitb32.pt\")\n",
        "\n",
        "print(\"Simulated checkpoints created:\")\n",
        "!ls checkpoints"
      ],
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "verify"
      },
      "source": [
        "## 6. Verify Checkpoint Files"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "list_ckpts"
      },
      "source": [
        "import os\n",
        "print(os.listdir(\"checkpoints\"))"
      ],
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "patch_task_list"
      },
      "source": [
        "## 7. Patch `TASK_CHECKPOINTS` in `run_light_experiment.py`\n",
        "Update the script to point to the checkpoint list if the filenames differ."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "patch_tasks"
      },
      "source": [
        "import re, json\n",
        "script_path = \"svd_hybrid_clip/run_light_experiment.py\"\n",
        "with open(script_path, \"r\") as f:\n",
        "    content = f.read()\n",
        "\n",
        "new_paths = [\n",
        "    \"checkpoints/cars_clip_vitb32.pt\",\n",
        "    \"checkpoints/eurosat_clip_vitb32.pt\",\n",
        "    \"checkpoints/dtd_clip_vitb32.pt\",\n",
        "    \"checkpoints/sun397_clip_vitb32.pt\"\n",
        "]\n",
        "new_block = \"TASK_CHECKPOINTS = \" + json.dumps(new_paths, indent=2)\n",
        "content = re.sub(r\"TASK_CHECKPOINTS\\s*=\\s*\\[.*?\\]\", new_block, content, flags=re.DOTALL)\n",
        "\n",
        "with open(script_path, \"w\") as f:\n",
        "    f.write(content)\n",
        "print(\"Updated TASK_CHECKPOINTS in run_light_experiment.py\")"
      ],
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "run_experiment"
      },
      "source": [
        "## 8. Run Light SVD-Hybrid Experiment\n",
        "Outputs:\n",
        "- Bases (U_high/U_low + meta)\n",
        "- Compressed per-task coefficient artifacts\n",
        "- Merged visual backbone state dict\n",
        "\n",
        "By default uses energy threshold 0.90 and max rank 32."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "execute_pipeline"
      },
      "source": [
        "!python svd_hybrid_clip/run_light_experiment.py --output-root ./svd_out --data-root ./data --device cuda"
      ],
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "inspect_artifacts"
      },
      "source": [
        "## 9. Inspect Artifacts"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "inspect1"
      },
      "source": [
        "import torch\n",
        "art = torch.load(\"svd_out/svd_hybrid_clip_artifacts.pt\")\n",
        "print(\"Layers stored:\", list(art[\"bases\"].keys()))\n",
        "print(\"Task order:\", art[\"task_order\"])\n",
        "for lname, (_, _, meta) in art[\"bases\"].items():\n",
        "    print(f\"Layer: {lname} | k={meta['k']} | N={meta['N']} | D={meta['D']}\")"
      ],
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "visualize_sv"
      },
      "source": [
        "## 10. (Optional) Visualize Singular Values"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sv_plot"
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "for lname, (_, _, meta) in art[\"bases\"].items():\n",
        "    sv = meta.get(\"singular_values\")\n",
        "    if sv is not None:\n",
        "        plt.figure(figsize=(4,3))\n",
        "        plt.plot(sv.numpy(), marker='o')\n",
        "        plt.title(f\"Top-k Singular Values: {lname[:35]}...\")\n",
        "        plt.xlabel(\"Index\")\n",
        "        plt.ylabel(\"Value\")\n",
        "        plt.show()\n",
        "        break  # show one example"
      ],
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "recon_errors"
      },
      "source": [
        "## 11. (Optional) Compute Reconstruction Errors Per Task & Layer\n",
        "Shows how well compressed coefficients reconstruct original deltas."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "reconstruction_loop"
      },
      "source": [
        "from svd_hybrid_clip.tvq_adapter import dequantize_low\n",
        "errors = {}\n",
        "bases = art[\"bases\"]\n",
        "artifacts = art[\"artifacts\"] if \"artifacts\" in art else art.get(\"artifacts\", {})\n",
        "# If artifacts not stored separately, adjust loading accordingly.\n",
        "for lname, (U_high, U_low, meta) in bases.items():\n",
        "    layer_art_list = artifacts.get(lname, [])\n",
        "    if not layer_art_list:\n",
        "        continue\n",
        "    layer_errs = []\n",
        "    # Original flat deltas were not saved here; approximate by reconstructing each task (for simulation only)\n",
        "    # If you want true error, modify pipeline to store original flats.\n",
        "    for art_task in layer_art_list:\n",
        "        c_high = art_task[\"c_high_fp16\"].float()\n",
        "        c_low = dequantize_low(art_task[\"c_low_quant\"]).float()\n",
        "        recon = U_high.float() @ c_high + U_low.float() @ c_low\n",
        "        # Here we can't compute true error without original delta; skip or assume original ~ recon for simulation.\n",
        "        layer_errs.append(0.0)\n",
        "    errors[lname] = layer_errs\n",
        "print(\"(Simulated) reconstruction error placeholders per layer:\")\n",
        "for lname, errs in errors.items():\n",
        "    if errs:\n",
        "        print(lname, \"mean=\", sum(errs)/len(errs))"
      ],
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "load_merged"
      },
      "source": [
        "## 12. (Optional) Load Merged Weights Into CLIP Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "apply_merged"
      },
      "source": [
        "merged_visual_sd = torch.load(\"svd_out/merged_visual_sd.pt\")\n",
        "model_merged, preprocess2 = clip.load(\"ViT-B/32\", device=\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "with torch.no_grad():\n",
        "    msd = model_merged.state_dict()\n",
        "    for k, v in merged_visual_sd.items():\n",
        "        if k in msd:\n",
        "            msd[k].copy_(v)\n",
        "print(\"Merged weights loaded into CLIP model.\")"
      ],
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "feature_demo"
      },
      "source": [
        "## 13. (Optional) Feature Extraction Demo"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "feature_extract"
      },
      "source": [
        "dummy = torch.randn(2, 3, 224, 224, device=\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "with torch.no_grad():\n",
        "    feats = model_merged.encode_image(dummy)\n",
        "print(\"Feature shape:\", feats.shape)"
      ],
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "next_steps"
      },
      "source": [
        "## 14. Next Steps\n",
        "- Replace simulated checkpoints with real fine-tuned ones.\n",
        "- Expand layer coverage (more transformer blocks).\n",
        "- Integrate real RTVQ (TVQ) quantizer in `tvq_adapter.py`.\n",
        "- Train a linear probe classifier on merged features per dataset.\n",
        "- Add weighted averaging of coefficients based on validation performance.\n",
        "- For large task counts, consider randomized SVD (`torch.svd_lowrank`)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "end"
      },
      "source": [
        "## Done\n",
        "You have run the light SVD-Hybrid merging pipeline in Colab. Adapt and extend for full experiments."
      ]
    }
  ]
}