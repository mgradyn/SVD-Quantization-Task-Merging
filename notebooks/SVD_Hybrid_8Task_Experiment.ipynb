{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "title"
   },
   "source": [
    "# SVD-Hybrid Task Merging Experiment - 8 Tasks on CLIP Models\n",
    "\n",
    "This notebook runs SVD-Hybrid merging experiments on 8 fine-tuned CLIP models.\n",
    "\n",
    "## Setup\n",
    "\n",
    "**Google Drive Structure Expected:**\n",
    "```\n",
    "My Drive/\n",
    "└── clip_finetune/\n",
    "    ├── ViT-B-16/\n",
    "    │   ├── Cars/finetuned.pt\n",
    "    │   ├── DTD/finetuned.pt\n",
    "    │   ├── EuroSAT/finetuned.pt\n",
    "    │   ├── GTSRB/finetuned.pt\n",
    "    │   ├── MNIST/finetuned.pt\n",
    "    │   ├── RESISC45/finetuned.pt\n",
    "    │   ├── SUN397/finetuned.pt\n",
    "    │   ├── SVHN/finetuned.pt\n",
    "    │   ├── Cars_head.pt\n",
    "    │   ├── DTD_head.pt\n",
    "    │   └── ... (head files for each dataset)\n",
    "    ├── ViT-B-32/\n",
    "    │   └── ... (same structure)\n",
    "    └── ViT-L-14/\n",
    "        └── ... (same structure)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "install-section"
   },
   "source": [
    "## 1. Installation & Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "install-deps"
   },
   "outputs": [],
   "source": [
    "# Install required dependencies\n",
    "!pip install torch torchvision numpy scikit-learn scipy open_clip_torch -q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "mount-drive"
   },
   "outputs": [],
   "source": [
    "# Mount Google Drive\n",
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "clone-repo"
   },
   "outputs": [],
   "source": [
    "# Clone the repository\n",
    "!git clone https://github.com/mgradyn/SVD-Quantization-Task-Merging.git\n",
    "%cd SVD-Quantization-Task-Merging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "imports"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import torch\n",
    "import json\n",
    "from pathlib import Path\n",
    "from typing import Dict, List, Optional\n",
    "\n",
    "# Add repository to path\n",
    "sys.path.insert(0, '/content/SVD-Quantization-Task-Merging')\n",
    "\n",
    "# Import SVD-Hybrid components\n",
    "from src.svd_hybrid.config import SVDHybridConfig\n",
    "from src.svd_hybrid.cli import run_svd_hybrid_pipeline\n",
    "from src.svd_hybrid.task_vector_loader import load_checkpoint, compute_task_vector\n",
    "from task_vectors import TaskVector\n",
    "from dataset_constants import STANDARD_8_TASKS, normalize_task_name\n",
    "\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "config-section"
   },
   "source": [
    "## 2. Configuration\n",
    "\n",
    "Configure paths and model settings. Adjust these based on your Google Drive setup."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "config"
   },
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# CONFIGURATION - Modify these paths based on your Google Drive structure\n",
    "# =============================================================================\n",
    "\n",
    "# Base path to checkpoints in Google Drive\n",
    "DRIVE_BASE_PATH = \"/content/drive/MyDrive/clip_finetune\"\n",
    "\n",
    "# Available models (choose one)\n",
    "AVAILABLE_MODELS = [\"ViT-B-16\", \"ViT-B-32\", \"ViT-L-14\"]\n",
    "\n",
    "# Select model to use for this experiment\n",
    "SELECTED_MODEL = \"ViT-B-32\"  # Change to ViT-B-16 or ViT-L-14 as needed\n",
    "\n",
    "# The 8 standard evaluation tasks\n",
    "TASKS = [\"Cars\", \"DTD\", \"EuroSAT\", \"GTSRB\", \"MNIST\", \"RESISC45\", \"SUN397\", \"SVHN\"]\n",
    "\n",
    "# Output directory (in Colab's local storage)\n",
    "OUTPUT_DIR = \"/content/output\"\n",
    "\n",
    "# Device configuration\n",
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "print(f\"Selected model: {SELECTED_MODEL}\")\n",
    "print(f\"Tasks: {TASKS}\")\n",
    "print(f\"Device: {DEVICE}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "path-setup"
   },
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# PATH SETUP - Construct paths based on Google Drive structure\n",
    "# =============================================================================\n",
    "\n",
    "def get_model_paths(base_path: str, model_name: str, tasks: List[str]) -> dict:\n",
    "    \"\"\"\n",
    "    Get all checkpoint and head paths for a model.\n",
    "    \n",
    "    Expected structure:\n",
    "    - clip_finetune/{model_name}/{task}/finetuned.pt\n",
    "    - clip_finetune/{model_name}/{task}_head.pt (or similar naming)\n",
    "    \"\"\"\n",
    "    model_dir = os.path.join(base_path, model_name)\n",
    "    \n",
    "    paths = {\n",
    "        \"model_dir\": model_dir,\n",
    "        \"checkpoints\": {},\n",
    "        \"heads\": {},\n",
    "    }\n",
    "    \n",
    "    for task in tasks:\n",
    "        # Finetuned checkpoint path\n",
    "        checkpoint_path = os.path.join(model_dir, task, \"finetuned.pt\")\n",
    "        paths[\"checkpoints\"][task] = checkpoint_path\n",
    "        \n",
    "        # Head path (try different naming conventions)\n",
    "        possible_head_paths = [\n",
    "            os.path.join(model_dir, f\"{task}_head.pt\"),\n",
    "            os.path.join(model_dir, f\"{task.lower()}_head.pt\"),\n",
    "            os.path.join(model_dir, f\"head_{task}.pt\"),\n",
    "            os.path.join(model_dir, task, \"head.pt\"),\n",
    "        ]\n",
    "        \n",
    "        for head_path in possible_head_paths:\n",
    "            if os.path.exists(head_path):\n",
    "                paths[\"heads\"][task] = head_path\n",
    "                break\n",
    "    \n",
    "    return paths\n",
    "\n",
    "# Get paths for selected model\n",
    "model_paths = get_model_paths(DRIVE_BASE_PATH, SELECTED_MODEL, TASKS)\n",
    "\n",
    "print(f\"\\nModel directory: {model_paths['model_dir']}\")\n",
    "print(f\"\\nCheckpoint paths:\")\n",
    "for task, path in model_paths[\"checkpoints\"].items():\n",
    "    exists = \"✓\" if os.path.exists(path) else \"✗\"\n",
    "    print(f\"  {exists} {task}: {path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "verify-paths"
   },
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# VERIFY ALL PATHS EXIST\n",
    "# =============================================================================\n",
    "\n",
    "def verify_paths(model_paths: dict) -> bool:\n",
    "    \"\"\"\n",
    "    Verify all required paths exist.\n",
    "    \"\"\"\n",
    "    all_valid = True\n",
    "    \n",
    "    print(\"Verifying checkpoint paths...\")\n",
    "    for task, path in model_paths[\"checkpoints\"].items():\n",
    "        if os.path.exists(path):\n",
    "            size_mb = os.path.getsize(path) / (1024 * 1024)\n",
    "            print(f\"  ✓ {task}: {size_mb:.1f} MB\")\n",
    "        else:\n",
    "            print(f\"  ✗ {task}: NOT FOUND at {path}\")\n",
    "            all_valid = False\n",
    "    \n",
    "    print(\"\\nVerifying head paths...\")\n",
    "    for task, path in model_paths.get(\"heads\", {}).items():\n",
    "        if os.path.exists(path):\n",
    "            size_mb = os.path.getsize(path) / (1024 * 1024)\n",
    "            print(f\"  ✓ {task} head: {size_mb:.2f} MB\")\n",
    "        else:\n",
    "            print(f\"  ✗ {task} head: NOT FOUND\")\n",
    "    \n",
    "    # Check for any head files in the model directory\n",
    "    if os.path.exists(model_paths[\"model_dir\"]):\n",
    "        print(\"\\nAll .pt files in model directory:\")\n",
    "        for f in sorted(os.listdir(model_paths[\"model_dir\"])):\n",
    "            if f.endswith(\".pt\"):\n",
    "                full_path = os.path.join(model_paths[\"model_dir\"], f)\n",
    "                size_mb = os.path.getsize(full_path) / (1024 * 1024)\n",
    "                print(f\"  - {f}: {size_mb:.2f} MB\")\n",
    "    \n",
    "    return all_valid\n",
    "\n",
    "paths_valid = verify_paths(model_paths)\n",
    "\n",
    "if not paths_valid:\n",
    "    print(\"\\n⚠️ Some paths are missing. Please verify your Google Drive structure.\")\n",
    "else:\n",
    "    print(\"\\n✓ All paths verified successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "base-model-section"
   },
   "source": [
    "## 3. Load Base Model\n",
    "\n",
    "We need the base (pretrained) CLIP model to compute task vectors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "load-base-model"
   },
   "outputs": [],
   "source": [
    "import open_clip\n",
    "\n",
    "def get_clip_model_name(model_name: str) -> tuple:\n",
    "    \"\"\"\n",
    "    Convert model name to open_clip format.\n",
    "    \"\"\"\n",
    "    model_map = {\n",
    "        \"ViT-B-16\": (\"ViT-B-16\", \"openai\"),\n",
    "        \"ViT-B-32\": (\"ViT-B-32\", \"openai\"),\n",
    "        \"ViT-L-14\": (\"ViT-L-14\", \"openai\"),\n",
    "    }\n",
    "    return model_map.get(model_name, (model_name, \"openai\"))\n",
    "\n",
    "def load_base_clip_model(model_name: str, device: str = \"cpu\"):\n",
    "    \"\"\"\n",
    "    Load the base (pretrained) CLIP model.\n",
    "    \"\"\"\n",
    "    clip_name, pretrained = get_clip_model_name(model_name)\n",
    "    print(f\"Loading base model: {clip_name} ({pretrained})...\")\n",
    "    \n",
    "    model, _, preprocess = open_clip.create_model_and_transforms(\n",
    "        clip_name, \n",
    "        pretrained=pretrained,\n",
    "        device=device\n",
    "    )\n",
    "    \n",
    "    # Get the state dict (visual encoder only for task vectors)\n",
    "    base_state_dict = model.visual.state_dict()\n",
    "    \n",
    "    print(f\"Loaded base model with {len(base_state_dict)} parameters\")\n",
    "    return model, base_state_dict, preprocess\n",
    "\n",
    "# Load base model\n",
    "base_model, base_state_dict, preprocess = load_base_clip_model(SELECTED_MODEL, DEVICE)\n",
    "\n",
    "# Save base state dict for task vector computation\n",
    "BASE_MODEL_PATH = \"/content/base_model.pt\"\n",
    "torch.save(base_state_dict, BASE_MODEL_PATH)\n",
    "print(f\"\\nBase model saved to: {BASE_MODEL_PATH}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "task-vectors-section"
   },
   "source": [
    "## 4. Load Task Vectors\n",
    "\n",
    "Compute task vectors (delta = finetuned - base) for each task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "load-task-vectors"
   },
   "outputs": [],
   "source": [
    "def load_task_vectors_from_checkpoints(\n",
    "    base_state_dict: Dict[str, torch.Tensor],\n",
    "    checkpoint_paths: Dict[str, str],\n",
    "    device: str = \"cpu\"\n",
    ") -> Dict[str, Dict[str, torch.Tensor]]:\n",
    "    \"\"\"\n",
    "    Load task vectors from finetuned checkpoints.\n",
    "    \n",
    "    Args:\n",
    "        base_state_dict: Base model state dict\n",
    "        checkpoint_paths: Dict mapping task names to checkpoint paths\n",
    "        device: Device to load to\n",
    "    \n",
    "    Returns:\n",
    "        Dict mapping task names to task vectors (delta dicts)\n",
    "    \"\"\"\n",
    "    task_vectors = {}\n",
    "    \n",
    "    for task, path in checkpoint_paths.items():\n",
    "        if not os.path.exists(path):\n",
    "            print(f\"  ⚠️ Skipping {task}: checkpoint not found\")\n",
    "            continue\n",
    "        \n",
    "        print(f\"  Loading {task}...\")\n",
    "        \n",
    "        try:\n",
    "            # Load finetuned checkpoint\n",
    "            finetuned_state = torch.load(path, map_location=device)\n",
    "            \n",
    "            # Handle nested state dicts\n",
    "            if isinstance(finetuned_state, dict):\n",
    "                if \"state_dict\" in finetuned_state:\n",
    "                    finetuned_state = finetuned_state[\"state_dict\"]\n",
    "                elif \"model\" in finetuned_state:\n",
    "                    finetuned_state = finetuned_state[\"model\"]\n",
    "                elif \"visual\" in finetuned_state:\n",
    "                    finetuned_state = finetuned_state[\"visual\"]\n",
    "            \n",
    "            # Compute task vector\n",
    "            task_vector = compute_task_vector(base_state_dict, finetuned_state, device)\n",
    "            \n",
    "            if len(task_vector) > 0:\n",
    "                task_vectors[task] = task_vector\n",
    "                print(f\"    ✓ {len(task_vector)} parameters\")\n",
    "            else:\n",
    "                print(f\"    ⚠️ No matching parameters found\")\n",
    "                \n",
    "        except Exception as e:\n",
    "            print(f\"    ✗ Error: {e}\")\n",
    "    \n",
    "    return task_vectors\n",
    "\n",
    "print(\"Loading task vectors...\\n\")\n",
    "task_vectors = load_task_vectors_from_checkpoints(\n",
    "    base_state_dict, \n",
    "    model_paths[\"checkpoints\"], \n",
    "    DEVICE\n",
    ")\n",
    "\n",
    "print(f\"\\nLoaded {len(task_vectors)} task vectors\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "analyze-task-vectors"
   },
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# ANALYZE TASK VECTORS\n",
    "# =============================================================================\n",
    "\n",
    "def analyze_task_vectors(task_vectors: Dict[str, Dict[str, torch.Tensor]]):\n",
    "    \"\"\"\n",
    "    Analyze task vectors to understand their characteristics.\n",
    "    \"\"\"\n",
    "    print(\"Task Vector Analysis\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    for task_name, task_vector in task_vectors.items():\n",
    "        total_params = sum(t.numel() for t in task_vector.values())\n",
    "        total_size_mb = sum(t.numel() * t.element_size() for t in task_vector.values()) / (1024 * 1024)\n",
    "        \n",
    "        # Compute statistics\n",
    "        all_values = torch.cat([t.flatten() for t in task_vector.values()])\n",
    "        mean_val = all_values.mean().item()\n",
    "        std_val = all_values.std().item()\n",
    "        max_val = all_values.abs().max().item()\n",
    "        \n",
    "        print(f\"\\n{task_name}:\")\n",
    "        print(f\"  Parameters: {len(task_vector)}\")\n",
    "        print(f\"  Total elements: {total_params:,}\")\n",
    "        print(f\"  Size: {total_size_mb:.2f} MB\")\n",
    "        print(f\"  Mean: {mean_val:.6f}\")\n",
    "        print(f\"  Std: {std_val:.6f}\")\n",
    "        print(f\"  Max abs: {max_val:.6f}\")\n",
    "\n",
    "if task_vectors:\n",
    "    analyze_task_vectors(task_vectors)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "heads-section"
   },
   "source": [
    "## 5. Load Classification Heads\n",
    "\n",
    "Load the task-specific classification heads for evaluation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "load-heads"
   },
   "outputs": [],
   "source": [
    "def find_and_load_heads(model_dir: str, tasks: List[str], device: str = \"cpu\"):\n",
    "    \"\"\"\n",
    "    Find and load classification heads for all tasks.\n",
    "    \"\"\"\n",
    "    heads = {}\n",
    "    \n",
    "    if not os.path.exists(model_dir):\n",
    "        print(f\"Model directory not found: {model_dir}\")\n",
    "        return heads\n",
    "    \n",
    "    # List all .pt files in model directory\n",
    "    pt_files = [f for f in os.listdir(model_dir) if f.endswith(\".pt\")]\n",
    "    \n",
    "    print(\"Available .pt files:\")\n",
    "    for f in sorted(pt_files):\n",
    "        print(f\"  - {f}\")\n",
    "    \n",
    "    print(\"\\nLoading heads...\")\n",
    "    for task in tasks:\n",
    "        # Try different naming patterns\n",
    "        possible_names = [\n",
    "            f\"{task}_head.pt\",\n",
    "            f\"{task.lower()}_head.pt\",\n",
    "            f\"head_{task}.pt\",\n",
    "            f\"head_{task.lower()}.pt\",\n",
    "            f\"{task}Head.pt\",\n",
    "            f\"{task}_classifier.pt\",\n",
    "        ]\n",
    "        \n",
    "        head_loaded = False\n",
    "        for name in possible_names:\n",
    "            path = os.path.join(model_dir, name)\n",
    "            if os.path.exists(path):\n",
    "                try:\n",
    "                    head = torch.load(path, map_location=device)\n",
    "                    heads[task] = head\n",
    "                    print(f\"  ✓ {task}: {name}\")\n",
    "                    head_loaded = True\n",
    "                    break\n",
    "                except Exception as e:\n",
    "                    print(f\"  ✗ {task}: Error loading {name}: {e}\")\n",
    "        \n",
    "        if not head_loaded:\n",
    "            print(f\"  ⚠️ {task}: No head found\")\n",
    "    \n",
    "    return heads\n",
    "\n",
    "# Load heads\n",
    "heads = find_and_load_heads(model_paths[\"model_dir\"], TASKS, DEVICE)\n",
    "print(f\"\\nLoaded {len(heads)} classification heads\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "experiment-section"
   },
   "source": [
    "## 6. Run SVD-Hybrid Merging Experiment\n",
    "\n",
    "Now we run the SVD-Hybrid merging with different configurations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "setup-checkpoint-dir"
   },
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# SETUP CHECKPOINT DIRECTORY FOR SVD-HYBRID\n",
    "# =============================================================================\n",
    "\n",
    "# Create a checkpoint directory with the expected structure\n",
    "CHECKPOINT_DIR = \"/content/checkpoints\"\n",
    "os.makedirs(CHECKPOINT_DIR, exist_ok=True)\n",
    "\n",
    "# Create symlinks or copy checkpoints to expected locations\n",
    "for task in TASKS:\n",
    "    source = model_paths[\"checkpoints\"][task]\n",
    "    task_dir = os.path.join(CHECKPOINT_DIR, task)\n",
    "    os.makedirs(task_dir, exist_ok=True)\n",
    "    \n",
    "    dest = os.path.join(task_dir, \"finetuned.pt\")\n",
    "    \n",
    "    if os.path.exists(source) and not os.path.exists(dest):\n",
    "        # Create symlink to avoid copying large files\n",
    "        os.symlink(source, dest)\n",
    "        print(f\"✓ Linked {task}\")\n",
    "    elif os.path.exists(dest):\n",
    "        print(f\"✓ {task} already linked\")\n",
    "    else:\n",
    "        print(f\"✗ {task} source not found\")\n",
    "\n",
    "print(f\"\\nCheckpoint directory: {CHECKPOINT_DIR}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "run-svd-hybrid"
   },
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# RUN SVD-HYBRID MERGING\n",
    "# =============================================================================\n",
    "\n",
    "def run_experiment(\n",
    "    tasks: List[str],\n",
    "    checkpoint_dir: str,\n",
    "    base_model_path: str,\n",
    "    output_dir: str,\n",
    "    config_name: str = \"default\",\n",
    "    energy_threshold: float = 0.95,\n",
    "    max_rank: int = 64,\n",
    "    weighting: str = \"uniform\",\n",
    "    device: str = \"cuda\"\n",
    "):\n",
    "    \"\"\"\n",
    "    Run SVD-Hybrid merging experiment with given configuration.\n",
    "    \"\"\"\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"Running experiment: {config_name}\")\n",
    "    print(f\"{'='*60}\")\n",
    "    print(f\"  Energy threshold: {energy_threshold}\")\n",
    "    print(f\"  Max rank: {max_rank}\")\n",
    "    print(f\"  Weighting: {weighting}\")\n",
    "    print(f\"  Tasks: {tasks}\")\n",
    "    \n",
    "    # Create output directory for this experiment\n",
    "    exp_output_dir = os.path.join(output_dir, config_name)\n",
    "    os.makedirs(exp_output_dir, exist_ok=True)\n",
    "    \n",
    "    # Create configuration\n",
    "    config = SVDHybridConfig(\n",
    "        tasks=tasks,\n",
    "        model=SELECTED_MODEL,\n",
    "        checkpoint_dir=checkpoint_dir,\n",
    "        base_model_path=base_model_path,\n",
    "        mask_dir=\"\",  # No masks for this experiment\n",
    "        svd_energy_threshold=energy_threshold,\n",
    "        svd_max_rank=max_rank,\n",
    "        svd_center=True,\n",
    "        svd_fp16=True,\n",
    "        svd_low_bits=4,\n",
    "        svd_rtvq_stages=2,\n",
    "        svd_weighting=weighting,\n",
    "        svd_store_artifacts=True,\n",
    "        svd_eval_reconstruction=True,\n",
    "        output_dir=exp_output_dir,\n",
    "        artifact_dir=os.path.join(exp_output_dir, \"artifacts\"),\n",
    "        device=device\n",
    "    )\n",
    "    \n",
    "    # Run pipeline\n",
    "    try:\n",
    "        results = run_svd_hybrid_pipeline(config)\n",
    "        print(f\"\\n✓ Experiment '{config_name}' completed successfully!\")\n",
    "        print(f\"  Output saved to: {exp_output_dir}\")\n",
    "        return results\n",
    "    except Exception as e:\n",
    "        print(f\"\\n✗ Experiment '{config_name}' failed: {e}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "        return None\n",
    "\n",
    "# Filter tasks to only those with valid checkpoints\n",
    "valid_tasks = [t for t in TASKS if t in task_vectors]\n",
    "print(f\"Valid tasks with checkpoints: {valid_tasks}\")\n",
    "\n",
    "# Run default experiment\n",
    "if len(valid_tasks) >= 2:\n",
    "    results_default = run_experiment(\n",
    "        tasks=valid_tasks,\n",
    "        checkpoint_dir=CHECKPOINT_DIR,\n",
    "        base_model_path=BASE_MODEL_PATH,\n",
    "        output_dir=OUTPUT_DIR,\n",
    "        config_name=\"uniform_e95_r64\",\n",
    "        energy_threshold=0.95,\n",
    "        max_rank=64,\n",
    "        weighting=\"uniform\",\n",
    "        device=DEVICE\n",
    "    )\n",
    "else:\n",
    "    print(\"⚠️ Not enough valid tasks to run experiment. Need at least 2 tasks.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "run-experiments-sweep"
   },
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# RUN EXPERIMENT SWEEP (OPTIONAL)\n",
    "# =============================================================================\n",
    "\n",
    "RUN_SWEEP = False  # Set to True to run multiple configurations (resource intensive)\n",
    "\n",
    "if RUN_SWEEP and len(valid_tasks) >= 2:\n",
    "    # Define experiment configurations\n",
    "    experiments = [\n",
    "        {\"name\": \"uniform_e90_r32\", \"energy\": 0.90, \"rank\": 32, \"weight\": \"uniform\"},\n",
    "        {\"name\": \"uniform_e95_r64\", \"energy\": 0.95, \"rank\": 64, \"weight\": \"uniform\"},\n",
    "        {\"name\": \"uniform_e99_r128\", \"energy\": 0.99, \"rank\": 128, \"weight\": \"uniform\"},\n",
    "    ]\n",
    "    \n",
    "    all_results = {}\n",
    "    \n",
    "    for exp in experiments:\n",
    "        results = run_experiment(\n",
    "            tasks=valid_tasks,\n",
    "            checkpoint_dir=CHECKPOINT_DIR,\n",
    "            base_model_path=BASE_MODEL_PATH,\n",
    "            output_dir=OUTPUT_DIR,\n",
    "            config_name=exp[\"name\"],\n",
    "            energy_threshold=exp[\"energy\"],\n",
    "            max_rank=exp[\"rank\"],\n",
    "            weighting=exp[\"weight\"],\n",
    "            device=DEVICE\n",
    "        )\n",
    "        \n",
    "        if results is not None:\n",
    "            all_results[exp[\"name\"]] = results\n",
    "    \n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"Completed {len(all_results)}/{len(experiments)} experiments\")\n",
    "    print(f\"{'='*60}\")\n",
    "else:\n",
    "    print(\"Skipping experiment sweep\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "simple-merge-section"
   },
   "source": [
    "## 7. Simple Task Arithmetic Baseline (Optional)\n",
    "\n",
    "For comparison, we also perform simple task arithmetic (averaging task vectors)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "simple-merge"
   },
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# SIMPLE TASK ARITHMETIC BASELINE\n",
    "# =============================================================================\n",
    "\n",
    "def simple_task_arithmetic(\n",
    "    base_state_dict: Dict[str, torch.Tensor],\n",
    "    task_vectors: Dict[str, Dict[str, torch.Tensor]],\n",
    "    scaling_factor: float = 1.0\n",
    ") -> Dict[str, torch.Tensor]:\n",
    "    \"\"\"\n",
    "    Simple task arithmetic: average task vectors and add to base model.\n",
    "    \n",
    "    merged = base + scaling_factor * mean(task_vectors)\n",
    "    \"\"\"\n",
    "    # Get all parameter names\n",
    "    all_params = set()\n",
    "    for tv in task_vectors.values():\n",
    "        all_params.update(tv.keys())\n",
    "    \n",
    "    # Average task vectors\n",
    "    merged_state = {}\n",
    "    n_tasks = len(task_vectors)\n",
    "    \n",
    "    for param_name in base_state_dict.keys():\n",
    "        base_param = base_state_dict[param_name]\n",
    "        \n",
    "        if param_name in all_params:\n",
    "            # Compute average delta\n",
    "            deltas = [tv[param_name] for tv in task_vectors.values() if param_name in tv]\n",
    "            avg_delta = sum(deltas) / len(deltas)\n",
    "            \n",
    "            # Apply to base\n",
    "            merged_state[param_name] = base_param + scaling_factor * avg_delta\n",
    "        else:\n",
    "            # Keep base value\n",
    "            merged_state[param_name] = base_param\n",
    "    \n",
    "    return merged_state\n",
    "\n",
    "# Run simple task arithmetic\n",
    "if task_vectors:\n",
    "    print(\"Running simple task arithmetic baseline...\")\n",
    "    \n",
    "    for scaling in [0.3, 0.5, 1.0]:\n",
    "        merged_simple = simple_task_arithmetic(\n",
    "            base_state_dict, \n",
    "            task_vectors, \n",
    "            scaling_factor=scaling\n",
    "        )\n",
    "        \n",
    "        # Save merged model\n",
    "        simple_output_path = os.path.join(OUTPUT_DIR, f\"simple_merge_scale{scaling}.pt\")\n",
    "        os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
    "        torch.save(merged_simple, simple_output_path)\n",
    "        print(f\"  ✓ Saved simple merge (scale={scaling}) to: {simple_output_path}\")\n",
    "else:\n",
    "    print(\"⚠️ No task vectors loaded, skipping simple merge\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "results-section"
   },
   "source": [
    "## 8. View Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "view-results"
   },
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# VIEW EXPERIMENT RESULTS\n",
    "# =============================================================================\n",
    "\n",
    "def load_diagnostics(output_dir: str, config_name: str) -> Optional[dict]:\n",
    "    \"\"\"\n",
    "    Load diagnostics from an experiment.\n",
    "    \"\"\"\n",
    "    diag_path = os.path.join(output_dir, config_name, \"artifacts\", \"diagnostics.json\")\n",
    "    \n",
    "    if os.path.exists(diag_path):\n",
    "        with open(diag_path, 'r') as f:\n",
    "            return json.load(f)\n",
    "    \n",
    "    # Try alternate path\n",
    "    diag_path = os.path.join(output_dir, config_name, \"diagnostics.json\")\n",
    "    if os.path.exists(diag_path):\n",
    "        with open(diag_path, 'r') as f:\n",
    "            return json.load(f)\n",
    "    \n",
    "    return None\n",
    "\n",
    "def print_experiment_summary(output_dir: str):\n",
    "    \"\"\"\n",
    "    Print summary of all experiments.\n",
    "    \"\"\"\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"EXPERIMENT RESULTS SUMMARY\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    if not os.path.exists(output_dir):\n",
    "        print(f\"Output directory not found: {output_dir}\")\n",
    "        return\n",
    "    \n",
    "    for exp_name in sorted(os.listdir(output_dir)):\n",
    "        exp_path = os.path.join(output_dir, exp_name)\n",
    "        \n",
    "        if not os.path.isdir(exp_path):\n",
    "            continue\n",
    "        \n",
    "        print(f\"\\n{exp_name}:\")\n",
    "        \n",
    "        # Load diagnostics\n",
    "        diagnostics = load_diagnostics(output_dir, exp_name)\n",
    "        \n",
    "        if diagnostics and \"summary\" in diagnostics:\n",
    "            summary = diagnostics[\"summary\"]\n",
    "            print(f\"  Num parameters: {summary.get('num_parameters', 'N/A')}\")\n",
    "            print(f\"  Avg rank: {summary.get('average_rank', 'N/A'):.1f}\")\n",
    "            print(f\"  Avg energy retained: {summary.get('average_energy_retained', 'N/A'):.4f}\")\n",
    "            print(f\"  Avg reconstruction error: {summary.get('average_reconstruction_error', 'N/A'):.6f}\")\n",
    "        \n",
    "        # Check if merged model exists\n",
    "        merged_path = os.path.join(exp_path, \"merged_state_dict.pt\")\n",
    "        if os.path.exists(merged_path):\n",
    "            size_mb = os.path.getsize(merged_path) / (1024 * 1024)\n",
    "            print(f\"  Merged model size: {size_mb:.2f} MB\")\n",
    "\n",
    "print_experiment_summary(OUTPUT_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "list-outputs"
   },
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# LIST ALL OUTPUT FILES\n",
    "# =============================================================================\n",
    "\n",
    "def list_directory_tree(path: str, prefix: str = \"\", max_depth: int = 3, current_depth: int = 0):\n",
    "    \"\"\"\n",
    "    Print directory tree.\n",
    "    \"\"\"\n",
    "    if current_depth > max_depth:\n",
    "        return\n",
    "    \n",
    "    if not os.path.exists(path):\n",
    "        return\n",
    "    \n",
    "    items = sorted(os.listdir(path))\n",
    "    \n",
    "    for i, item in enumerate(items):\n",
    "        is_last = (i == len(items) - 1)\n",
    "        connector = \"└── \" if is_last else \"├── \"\n",
    "        \n",
    "        item_path = os.path.join(path, item)\n",
    "        \n",
    "        if os.path.isdir(item_path):\n",
    "            print(f\"{prefix}{connector}{item}/\")\n",
    "            extension = \"    \" if is_last else \"│   \"\n",
    "            list_directory_tree(item_path, prefix + extension, max_depth, current_depth + 1)\n",
    "        else:\n",
    "            size_mb = os.path.getsize(item_path) / (1024 * 1024)\n",
    "            print(f\"{prefix}{connector}{item} ({size_mb:.2f} MB)\")\n",
    "\n",
    "print(\"\\nOutput Directory Structure:\")\n",
    "print(\"=\"*60)\n",
    "if os.path.exists(OUTPUT_DIR):\n",
    "    list_directory_tree(OUTPUT_DIR)\n",
    "else:\n",
    "    print(f\"Output directory not found: {OUTPUT_DIR}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "save-section"
   },
   "source": [
    "## 9. Save Results to Google Drive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "save-to-drive"
   },
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# SAVE RESULTS TO GOOGLE DRIVE\n",
    "# =============================================================================\n",
    "\n",
    "SAVE_TO_DRIVE = True  # Set to True to save results to Google Drive\n",
    "DRIVE_OUTPUT_DIR = f\"/content/drive/MyDrive/svd_hybrid_results/{SELECTED_MODEL}\"\n",
    "\n",
    "if SAVE_TO_DRIVE and os.path.exists(OUTPUT_DIR):\n",
    "    import shutil\n",
    "    \n",
    "    # Create output directory in Google Drive\n",
    "    os.makedirs(DRIVE_OUTPUT_DIR, exist_ok=True)\n",
    "    \n",
    "    # Copy results\n",
    "    print(f\"Saving results to Google Drive: {DRIVE_OUTPUT_DIR}\")\n",
    "    \n",
    "    for item in os.listdir(OUTPUT_DIR):\n",
    "        src = os.path.join(OUTPUT_DIR, item)\n",
    "        dst = os.path.join(DRIVE_OUTPUT_DIR, item)\n",
    "        \n",
    "        if os.path.isdir(src):\n",
    "            if os.path.exists(dst):\n",
    "                shutil.rmtree(dst)\n",
    "            shutil.copytree(src, dst)\n",
    "            print(f\"  ✓ Copied {item}/\")\n",
    "        else:\n",
    "            shutil.copy2(src, dst)\n",
    "            print(f\"  ✓ Copied {item}\")\n",
    "    \n",
    "    print(f\"\\n✓ Results saved to: {DRIVE_OUTPUT_DIR}\")\n",
    "else:\n",
    "    print(\"Skipping save to Google Drive\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cleanup-section"
   },
   "source": [
    "## 10. Cleanup (Optional)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "cleanup"
   },
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# CLEANUP (OPTIONAL)\n",
    "# =============================================================================\n",
    "\n",
    "CLEANUP = False  # Set to True to clean up temporary files\n",
    "\n",
    "if CLEANUP:\n",
    "    import shutil\n",
    "    \n",
    "    # Remove temporary directories\n",
    "    temp_dirs = [\n",
    "        CHECKPOINT_DIR,\n",
    "        OUTPUT_DIR,\n",
    "        \"/content/SVD-Quantization-Task-Merging\",\n",
    "    ]\n",
    "    \n",
    "    for d in temp_dirs:\n",
    "        if os.path.exists(d):\n",
    "            shutil.rmtree(d)\n",
    "            print(f\"Removed: {d}\")\n",
    "    \n",
    "    # Remove base model\n",
    "    if os.path.exists(BASE_MODEL_PATH):\n",
    "        os.remove(BASE_MODEL_PATH)\n",
    "        print(f\"Removed: {BASE_MODEL_PATH}\")\n",
    "    \n",
    "    print(\"\\n✓ Cleanup complete\")\n",
    "else:\n",
    "    print(\"Cleanup skipped. Set CLEANUP = True to remove temporary files.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "summary-section"
   },
   "source": [
    "## Summary\n",
    "\n",
    "This notebook performed the following:\n",
    "\n",
    "1. **Setup**: Installed dependencies and mounted Google Drive\n",
    "2. **Configuration**: Set up paths for checkpoints in Google Drive\n",
    "3. **Base Model**: Loaded pretrained CLIP model\n",
    "4. **Task Vectors**: Computed task vectors from fine-tuned checkpoints\n",
    "5. **Heads**: Loaded classification heads for evaluation\n",
    "6. **SVD-Hybrid Merging**: Ran experiments with different configurations\n",
    "7. **Simple Baseline**: Performed simple task arithmetic for comparison\n",
    "8. **Results**: Displayed experiment results and diagnostics\n",
    "9. **Save**: Optionally saved results to Google Drive\n",
    "\n",
    "### Next Steps\n",
    "\n",
    "- Evaluate merged models on test datasets\n",
    "- Compare SVD-Hybrid results with baseline methods\n",
    "- Tune hyperparameters (energy threshold, max rank, etc.)\n",
    "- Try different weighting strategies (performance-based, cluster-based)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
